## WhisperX Transcriber

WhisperX ist ein modernes pre-trained-model mit zugrundeliegenden Transformer-Modellen, das in unserem Fall zur Transkription von Audioinhalten verwendet wird. Es ist ein Open-Source-Tool, das auf der Basis OpenAIs Whisper entwickelt wurde. 

Das Modell "large-v2" wird benutzt, um die Transkription durchzuführen. Hierbei handelt es sich um ein sehr großes Modell, das auf einer Vielzahl von Daten trainiert wurde und daher eine hohe Genauigkeit bei der Transkription von Audioinhalten aufweist. Der KI-Server ermöglicht es uns die Transkription auf den GPUs durchzuführen, was die Geschwindigkeit des Prozesses erhöht. Hierfür werden die globalen Variablen ```__DEVICE__```, ```__TYPE__``` und ```__BATCH_SIZE__``` verwendet. Das Modell wird mit den ```device_index=[0, 1, 2, 3]``` auf den GPUs parallelisiert. Die Audiodatei wird anschließend aus der Datenbank geladen und transkribiert. Danach wird zusammen mit dem Ausrichtungsmodell von WhisperX die Transkription auf Wortebene mit einem Zeitstempel versehen. Zuletzt passiert eine Rückgabe der einzelnen Segmente in einem kombinierten Dictonary- und Listenobjekt, dass alle nötigen Infos enthält.

## Audio classification

```audio_classification.py``` ist für das Trainieren eines Modells und einer Pipeline für die Klassifizierung von Audiodateien zuständig. Es verwendet die Bibliothek ```librosa``` für die Feature-Extraktion aud den Audiodateien und ```sklearn``` für das Trainieren des Modells. ```pandas``` wird für die Datenmanipulation und -speicherung verwendet, und ```pickle``` für das Speichern des Modells.

Nach dem Starten des skripts gibt es zuerst die Möglichkeit zu entscheiden, ob die Audiofeatures neu berechnet werden sollen oder ob bereits gespeicherte Features verwendet werden sollen. Bei einem Datensatz von 3596 Audiodateien, wovon ca. 90 % in leichter Sprache sind, hat die Extraktion mit librosa ca. 30 Minuten gedauert. Dementsprechend war die Implementierung einer Speicherfunktion für die Features sinnvoll. Je nachdem wird nun mithilfe von ```pickle``` entweder der Dataframe geladen oder ```load_audio_data()``` und ```extract_audio_features()``` aufgerufen. Diese Funktionen nutzen Teile des Datahandlers um sämtliche Pfade zu Audiodateien zu erhalten. Hier wurde zur Übersicht mit ```tqdm``` gearbeitet, damit eine Fortschrittsanzeige für die Iterationen der Extraktion zu sehen ist. Die Schlüsselfunktionen sind größtenteils Fehlerresistent damit es nicht zu einem Abbruch kommt und der Fortschritt verloren geht. 

Nun folgt das Training des Modells. Es wird unterschieden zwischen einem Training, das gleichzeitig zu Testzwecken und Evaluation des Modells dient und den Datensatz in einen Trainingsdatensatz und einen Testdatensatz teilt und einem reinen Training für das finale Modell, dass die besten Hyperparameter aus einem vorherigen getesteten Modell verwendet. Bei Tests mit einem KNN Modell stellte sich heraus, dass dieses sich schwertat die schweren Audios zu erkennen, deswegen fiel die Entscheidung nach einigen weiteren Tests auf einen Support Vector Classifier, hierbei wird in einer Pipeline zunächst ein ```MinMaxScaler``` für die normalisierung verwendet und anschließend die SVM. Eine GridSearch mit integrierter Cross-Validation wird durchgeführt, um die besten Hyperparameter zu finden.

Die beiden ungenutzten Funktionen ```extract_text_features()``` und ```load_and_split_audio()``` waren ursprünglich für die Implementierung eines weiteren Modells für die Textklassifizierung und die Aufteilung der Audios in kleiner Segmente gedacht. Diese wurden jedoch nicht weiter verfolgt, da die Klassifizierung der Audios in leichter und schwerer Sprache ausreichend war und keine Zeit für weiterführende Aufgaben mehr bestand.


## GUI Anwendung

Die GUI-Anwendung ist mit ```Flask``` erstellt worden. Sie ermöglicht es Nutzer*innen Audiodateien hochzuladen und diese zu transkribieren. 
1. **Importe und Konfiguration**: Nach einer Reihe von Importen und Konfigurationen wird die Flask-App initialisiert. Ein Secret Key wird gesetzt, um Sessions zu verwalten. Diese werden auf Serverseite verwaltet, um größere Datenübertragungen zu ermöglichen. Zusätzlich werden Data handler für die beiden Datenquellen initialisiert.
2. **WhisperX Modell**: Das WhisperX Modell wird bei der Audioklassifizierung geladen
3. **Flask-Routen**: Es gibt drei wesentliche Routen, die die Anwendung bereitstellt.
   - ```/``` ist die Startseite, die eine Begrüßungsnachricht anzeigt.
   - Die erste Route ```/upload``` ist die Seite, die die Möglichkeit bietet, eine Audiodatei hochzuladen.
   - Die zweite Route ```/transcribe``` erhält die Audiodatei von ```/upload``` und überprüft im ersten Schritt, ob eine Audiodatei hochgeladen wurde. Wenn eine Datei hochgeladen wurde, wird sie temporär auf dem Server gespeichert. Anschließend wird die Audiodatei an das WhisperX-Modell zur Transkription übergeben. Die Transkriptionsergebnisse werden in der ```results``` Variable gespeichert. Die transkribierten Segmente werden mit dem Ausrichtungsmodell von WhisperX auf Wortebene ausgerichtet. Dies ermöglicht es, Zeitstempel für jedes Wort in der Transkription zu erhalten. Nun werden die Audio-Features der Audiodatei extrahiert, um sie für die Klassifikation vorzubereiten. Das Klassifikationsmodell aus der ```audio_classification.py``` wird geladen und verwendet, um die Audiodatei zu klassifizieren. Die Klassifikationsergebnisse werden in der classification Variable gespeichert. Es findet eine Suche in den Datenbanken "dlf" und "mdr" nach dem transkribierten Titel des Artikels statt. Wenn der Titel gefunden wird, werden die Quelle und der Schwierigkeitsgrad des Artikels in der database Variable gespeichert, ansonsten wird er als "unknown" markiert. Die Transkriptionsergebnisse, die Verarbeitungszeit, die Datenbankinformationen und die Klassifikationsergebnisse werden in der Sitzung gespeichert.
   - ```/results``` zeigt die Transkriptionsergebnisse, die Verarbeitungszeit, die Datenbankinformationen und die Klassifikationsergebnisse in einerneinfachen Oberfläche an.